"""
èŠå¤©å¯¹è¯è·¯ç”±æ¨¡å—
å®ç°å¿ƒç†å’¨è¯¢å¯¹è¯æ ¸å¿ƒåŠŸèƒ½
é‡‡ç”¨æŠ€æœ¯ï¼šFastAPI + Qwen3-32Bå¤§è¯­è¨€æ¨¡å‹ + RAGæ£€ç´¢å¢å¼º + æƒ…ç»ªåˆ†æ
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Dict, Any
import time
from datetime import datetime
import json as json_module
from fastapi.responses import JSONResponse

from memory import get_user_memory
from emotion_analyzer import EmotionTracker
from crisis_detector import CrisisDetector, CRISIS_RESPONSE
from context_manager import ContextManager

# åˆ›å»ºè·¯ç”±å®ä¾‹
router = APIRouter(prefix="/api", tags=["chat"])

# å¯¼å…¥å…¨å±€å˜é‡ï¼ˆåœ¨main.pyä¸­åˆå§‹åŒ–ï¼‰
llm_pipe = None
embed_model = None
tokenizer = None
index = None
texts = None
emotion_tracker = None
crisis_detector = None

# ä¸Šä¸‹æ–‡ç®¡ç†å™¨å­—å…¸ï¼ˆæŒ‰ç”¨æˆ·IDå­˜å‚¨ï¼‰
context_managers = {}

class QueryRequest(BaseModel):
    query: str
    user_id: str = "anonymous"
    skip_crisis_check: bool = False

class QueryResponse(BaseModel):
    answer: str
    risk_level: str = "low"
    is_crisis: bool = False
    intervention_triggered: bool = False
    confidence: float = 0.0
    reason: str = ""
    reference_count: int = 0
    processing_time: float = 0.0

def embed_query(query: str):
    """å°†æŸ¥è¯¢æ–‡æœ¬ç¼–ç ä¸ºå‘é‡ï¼ˆCPUè¿è¡Œï¼‰"""
    import torch
    inputs = tokenizer([query], padding=True, truncation=True, 
                      return_tensors="pt", max_length=512)
    with torch.no_grad():
        outputs = embed_model(**inputs)
        embedding = outputs.last_hidden_state[:, 0]
        embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)
    return embedding.numpy().astype('float32')

@router.post("/ask", response_model=QueryResponse)
async def ask(request: QueryRequest):
    """å¿ƒç†å’¨è¯¢å¯¹è¯ä¸»æ¥å£"""
    global llm_pipe, embed_model, tokenizer, index, texts, emotion_tracker, crisis_detector
    
    # ç¡®ä¿å“åº”ä½¿ç”¨UTF-8ç¼–ç 
    start_time = time.time()
    user_query = request.query.strip()
    user_id = request.user_id

    if not user_query:
        raise HTTPException(status_code=400, detail="æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º")

    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] ç”¨æˆ· {user_id[:8]}...: {user_query[:40]}...")

    # åˆå§‹åŒ–ç”¨æˆ·è®°å¿†
    user_memory = get_user_memory(user_id)

    try:
        # ========== ç¬¬1æ­¥ï¼šæƒ…ç»ªåˆ†æ ==========
        emotion_result = emotion_tracker.track_user_emotion(user_id, user_query)
        print(f"ğŸ˜Š æƒ…ç»ªåˆ†æ: {emotion_result['emotion']} (ç½®ä¿¡åº¦: {emotion_result['confidence']:.2f})")
        
        # ========== ç¬¬2æ­¥ï¼šå±æœºæ£€æµ‹ ==========
        if not request.skip_crisis_check:
            risk_result = crisis_detector.detect(user_query, user_id)
            print(f"ğŸ” é£é™©è¯„ä¼°: {risk_result['level']} (score: {risk_result['score']:.2f})")

            if risk_result["level"] == "high":
                # è®°å½•åˆ°è®°å¿†
                user_memory.add_conversation(
                    user_query, 
                    CRISIS_RESPONSE["high"], 
                    "high",
                    risk_result["score"],
                    0
                )

                return QueryResponse(
                    answer=CRISIS_RESPONSE["high"],
                    risk_level="high",
                    is_crisis=True,
                    intervention_triggered=True,
                    confidence=risk_result["score"],
                    reason=risk_result["reason"],
                    reference_count=0,
                    processing_time=time.time() - start_time
                )
        else:
            risk_result = {"level": "low", "score": 0.0, "reason": "æ£€æµ‹å·²è·³è¿‡"}

        # ========== ç¬¬3æ­¥ï¼šè·å–è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†ï¼‰==========
        # è·å–æˆ–åˆ›å»ºè¯¥ç”¨æˆ·çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        if user_id not in context_managers:
            context_managers[user_id] = ContextManager(max_tokens=128000)  # 128K tokené™åˆ¶
        
        context_manager = context_managers[user_id]
        
        # ä»ç”¨æˆ·è®°å¿†ä¸­è·å–æ›´å¤šå†å²å¯¹è¯ç”¨äºåˆå§‹åŒ–
        memory_context_raw = user_memory.get_recent_context(max_turns=10)
        profile_summary = user_memory.get_profile_summary()
        
        # å¦‚æœæ˜¯æ–°ä¼šè¯ï¼Œå¯ä»¥ä»å†å²è®°å¿†åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        if len(context_manager.context_window) == 0 and memory_context_raw:
            # è§£æå†å²å¯¹è¯å¹¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„è§£æé€»è¾‘
            pass
        
        # è·å–æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡
        memory_context = context_manager.get_formatted_context(max_turns=5)  # æœ€å¤šå–5è½®
        
        if memory_context:
            stats = context_manager.get_statistics()
            print(f"ğŸ“š ä¸Šä¸‹æ–‡ç®¡ç†: {stats['total_turns']} è½®, {stats['total_tokens']} tokens, "
                  f"åˆ©ç”¨ç‡: {stats['utilization_rate']}%")

        # ========== ç¬¬4æ­¥ï¼šRAGæ£€ç´¢ ==========
        query_embedding = embed_query(user_query)
        distances, indices = index.search(query_embedding, k=3)

        contexts = [texts[idx] for idx in indices[0]]
        context = "\n\n---\n".join(contexts)

        # ========== ç¬¬5æ­¥ï¼šæ„é€ å¢å¼ºPrompt ==========
        memory_section = ""
        if profile_summary and "æ–°ç”¨æˆ·" not in profile_summary:
            memory_section = f"""ã€ç”¨æˆ·ç”»åƒã€‘{profile_summary}\n"""
            if memory_context:
                memory_section += f"""ã€è¿‘æœŸå¯¹è¯å†å²ã€‘\n{memory_context}\n\n"""

        safety_hint = ""
        if risk_result["level"] == "medium":
            safety_hint = "ï¼ˆæ³¨æ„ï¼šç”¨æˆ·æƒ…ç»ªè¾ƒä½è½ï¼Œè¯·ç‰¹åˆ«ç»™äºˆæ¸©æš–æ”¯æŒï¼‰\n"

        # åå°æ·±åº¦æ€è€ƒï¼ˆæ¶ˆè€—tokenä½†ä¸å±•ç¤ºï¼‰
        thinking_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šå¿ƒç†å’¨è¯¢å¸ˆAIåŠ©æ‰‹ã€‚è¯·åˆ†æï¼š

ç”¨æˆ·é—®é¢˜ï¼š{user_query}
å†å²èƒŒæ™¯ï¼š{memory_context if memory_context else 'æ–°ç”¨æˆ·'}
å‚è€ƒæ¡ˆä¾‹ï¼š{context[:200]}...

åˆ†æç»´åº¦ï¼šé—®é¢˜æ ¸å¿ƒã€æƒ…ç»ªçŠ¶æ€ã€å»ºè®®è¦ç‚¹"""
        
        # æ‰§è¡Œåå°æ€è€ƒ
        thinking_result = llm_pipe(thinking_prompt, max_new_tokens=100, temperature=0.3)
        thinking_analysis = thinking_result.text.strip()
        print(f"ğŸ§  åå°æ€è€ƒå®Œæˆ ({len(thinking_analysis)}å­—ç¬¦)")
        
        # é¢å‘ç”¨æˆ·çš„æ­£å¼å›å¤prompt
        prompt = f"""{safety_hint}{memory_section}ä½ æ˜¯ä¸€ä½ä¸“ä¸šä¸”å¯Œæœ‰åŒç†å¿ƒçš„å¿ƒç†å’¨è¯¢å¸ˆã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_query}

é‡è¦è¦æ±‚ï¼š
1. å¿…é¡»ä½¿ç”¨çº¯ä¸­æ–‡å›å¤ï¼Œä¸è¦å¤¹æ‚è‹±æ–‡
2. è¯­æ°”æ¸©æš–ã€ä¸“ä¸šã€å…±æƒ…
3. ç›´æ¥å›åº”ç”¨æˆ·æ ¸å¿ƒå…³åˆ‡
4. æä¾›å…·ä½“å¯è¡Œçš„å»ºè®®

è¯·ç”¨ä¸­æ–‡ç»™å‡ºç®€æ´ã€æ¸©æš–ã€å®ç”¨çš„å›å¤ï¼š"""

        # ========== ç¬¬6æ­¥ï¼šç”Ÿæˆå›ç­” ==========
        response = llm_pipe(prompt, max_new_tokens=512, temperature=0.7, top_p=0.9)
        answer = response.text.strip()
        print(f"ğŸ¤– å¤§æ¨¡å‹ç”Ÿæˆå®Œæˆï¼Œå›å¤é•¿åº¦: {len(answer)} å­—ç¬¦")
        print(f"ğŸ¤– å›å¤é¢„è§ˆ: {answer[:100]}...")

        if risk_result["level"] == "medium":
            answer += "\n\n---\nğŸ’™ æ¸©é¦¨æç¤ºï¼šå¦‚æœä½ æ„Ÿåˆ°æŒç»­çš„æƒ…ç»ªå›°æ‰°ï¼Œå¯éšæ—¶æ‹¨æ‰“ **400-161-9995** ã€‚"

        # ========== ç¬¬7æ­¥ï¼šä¿å­˜åˆ°è®°å¿†å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨ ==========
        user_memory.add_conversation(
            user_query,
            answer,
            risk_result["level"],
            risk_result.get("semantic_score", 0.0),
            len(contexts)
        )
        
        # æ·»åŠ åˆ°æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        context_manager.add_turn(
            user_message=user_query,
            ai_response=answer,
            emotion_score=emotion_result["confidence"],
            keywords=emotion_result.get("keywords", [])
        )

        processing_time = time.time() - start_time
        print(f"âœ… å®Œæˆ ({processing_time:.2f}s)")

        # è¿”å›å®Œæ•´å“åº”
        response_data = {
            "answer": answer,
            "risk_level": risk_result["level"],
            "is_crisis": risk_result["level"] in ["high", "medium"],
            "intervention_triggered": False,
            "confidence": risk_result["score"],
            "reason": risk_result["reason"],
            "reference_count": len(contexts),
            "processing_time": processing_time,
            "emotion": emotion_result["emotion"],
            "emotion_confidence": emotion_result["confidence"],
            "emotion_details": emotion_result["all_probabilities"]
        }
        return JSONResponse(content=response_data, headers={"Content-Type": "application/json; charset=utf-8"})

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
